{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../../src/chung/\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import Layout, IntSlider, Image, Text, HBox, VBox, interact\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Visualizations\n",
    "\n",
    "This notebook aims to show how a predicted example changed throughout the training progress.\n",
    "\n",
    "### Music Sequence Modeling Visualizations\n",
    "\n",
    "We can see how the model learned to output a music piece throughout the training progress. Here's the code for visualizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_music_training(dataset_name, model_name, data_type):\n",
    "    music_dataset_configs = {'all': {'hbox': True, 'width': '45%'},\n",
    "                             'nott': {'hbox': False},\n",
    "                             'jsb': {'hbox': True, 'width': '45%'},\n",
    "                             'muse': {'hbox': False},\n",
    "                             'piano': {'hbox': False}}\n",
    "\n",
    "    pred_dir = f'../../src/chung/exp/music/{dataset_name}/{model_name}/img_{data_type}'\n",
    "    with open(f'../../src/chung/exp/music/{dataset_name}/{model_name}/img_{data_type}/gold.png', 'rb') as f:\n",
    "        img_gold = f.read()\n",
    "    wig_img_gold = Image(value=img_gold, format='png', **music_dataset_configs[dataset_name])\n",
    "\n",
    "    def visualizer(img_idx):\n",
    "        with open(os.path.join(pred_dir, f'{img_idx}.png'), 'rb') as f:\n",
    "            img_pred = f.read()\n",
    "        wig_img_pred = Image(value=img_pred, format='png', **music_dataset_configs[dataset_name])\n",
    "        if music_dataset_configs[dataset_name]['hbox']:\n",
    "            box = HBox([wig_img_pred, wig_img_gold])\n",
    "        else:\n",
    "            box = VBox([wig_img_pred, wig_img_gold])\n",
    "        display(box)\n",
    "\n",
    "    return visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `dataset_name` can be one of `['all', 'nott', 'jsb', 'muse', 'piano']`.\n",
    "* The `model_name` can be one of `['lstm', 'gru', 'tanh']`.\n",
    "* The `data_type` can be one of `['train', 'valid']`, which is an example of which particular dataset split.\n",
    "\n",
    "We use `interact` widget to visualize the change in the example as shown below. The slider can be moved to see results in latter epochs. The first visualized image is the predicted music sequence while the second visualized image is the ground truth music sequence.\n",
    "\n",
    "First, let's check the difference between GRU, LSTM, Tanh on the JSB validation set.\n",
    "\n",
    "#### GRU on JSB Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfe0a301e564bcca653885b146d2b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=70, description='Epoch:', layout=Layout(width='90%'), max=499), Output()…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visual_func = visualize_music_training('jsb', 'gru', 'valid')\n",
    "_ = interact(visual_func, img_idx=IntSlider(description='Epoch:',\n",
    "                            value=70, min=0, max=499, step=1,\n",
    "                            layout=Layout(width='90%')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM on JSB Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0091773050d2461ebe42f2261b791bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=70, description='Epoch:', layout=Layout(width='90%'), max=499), Output()…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visual_func = visualize_music_training('jsb', 'lstm', 'valid')\n",
    "_ = interact(visual_func, img_idx=IntSlider(description='Epoch:',\n",
    "                            value=70, min=0, max=499, step=1,\n",
    "                            layout=Layout(width='90%')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanh RNN on JSB Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6592bf2e06d140cdad909d9937cbbc68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=70, description='Epoch:', layout=Layout(width='90%'), max=499), Output()…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visual_func = visualize_music_training('jsb', 'tanh', 'valid')\n",
    "_ = interact(visual_func, img_idx=IntSlider(description='Epoch:',\n",
    "                            value=70, min=0, max=499, step=1,\n",
    "                            layout=Layout(width='90%')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that GRU has never been able to really learn the notes at the top while LSTM generalized somewhat well for the top notes. The top notes on the GRU predictions are quite dense when it should be sparser. On the other hand, Tanh have trouble generalizing the basic structure of the music (like chords) for most of the epochs.\n",
    "\n",
    "The notes that are singled out and not really connected with others are the hardest to learn for these models. It should be noted that LSTM seem to have generalized the structure of the music quite well, especially for the notes at the bottom.\n",
    "\n",
    "We can also see the difference for long music pieces by comparing LSTM with Tanh in the Nottingham dataset.\n",
    "\n",
    "#### LSTM on Nottingham Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc8a144b6e546558df69e893cfd2344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=221, description='Epoch:', layout=Layout(width='90%'), max=499), Output(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visual_func = visualize_music_training('nott', 'lstm', 'valid')\n",
    "_ = interact(visual_func, img_idx=IntSlider(description='Epoch:',\n",
    "                            value=221, min=0, max=499, step=1,\n",
    "                            layout=Layout(width='90%')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanh RNN on Nottingham Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9422d7264b4a4dfd9cf5378e52e46c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=221, description='Epoch:', layout=Layout(width='90%'), max=499), Output(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visual_func = visualize_music_training('nott', 'tanh', 'valid')\n",
    "_ = interact(visual_func, img_idx=IntSlider(description='Epoch:',\n",
    "                            value=221, min=0, max=499, step=1,\n",
    "                            layout=Layout(width='90%')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that LSTM has generalized the unique structure of the top notes quite well, whereas Tanh RNN is constantly confused about what the top notes should be. Checking around epoch 221 show how constant LSTM is predicting the top notes while Tanh RNN constantly changed.\n",
    "\n",
    "### Machine Translation Visualizations\n",
    "\n",
    "Let's also do some visualizations on the machine translation task. Here's the code for visualizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_translation_training(dataset_name, model_name, data_type):\n",
    "    pred_dir = f'../../src/chung/exp/translation/{dataset_name}/{model_name}/text_{data_type}'\n",
    "    with open(f'../../src/chung/exp/translation/{dataset_name}/{model_name}/text_{data_type}/gold.json', 'r') as f:\n",
    "        text_gold = json.load(f)\n",
    "    wig_text_src = Text(value=text_gold['src_text'], description=\"Source Text:\", layout=Layout(width=\"95%\"))\n",
    "    wig_text_gold = Text(value=text_gold['tgt_text'], description=\"Gold Text:\", layout=Layout(width=\"95%\"))\n",
    "\n",
    "    def visualizer(text_idx):\n",
    "        with open(os.path.join(pred_dir, f'{text_idx}.json'), 'r') as f:\n",
    "            text_pred = json.load(f)\n",
    "        wig_text_pred = Text(value=text_pred['tgt_text'], description=\"Pred Text:\", layout=Layout(width=\"95%\"))\n",
    "        wig_text_pred_nll = Text(value=str(text_pred['nll']), description=\"Pred NLL:\", layout=Layout(width=\"95%\"))\n",
    "        box = VBox([wig_text_src, wig_text_gold, wig_text_pred, wig_text_pred_nll])\n",
    "        display(box)\n",
    "\n",
    "    return visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `dataset_name` can be one of `['en_de', 'de_it', 'it_en']`.\n",
    "* The `model_name` can be one of `['lstm', 'gru', 'tanh']`.\n",
    "* The `data_type` can be one of `['train', 'valid']`, which is an example of which particular dataset split.\n",
    "\n",
    "The visualization will also output the negative log-likelihood of the predicted text at the epoch of training. The negative log-likelihood should also get lower here as the model is learning to increase the probability of the gold (target) text in the predicted text.\n",
    "\n",
    "Let's look at how GRU, LSTM, Tanh RNN trained the IWSLT2017 English to German dataset:\n",
    "\n",
    "#### GRU on IWSLT2017 English to German Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cceac65b11b42a488207d0f4105afa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Epoch:', layout=Layout(width='90%'), max=99), Output()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visual_func = visualize_translation_training('en_de', 'gru', 'valid')\n",
    "_ = interact(visual_func, text_idx=IntSlider(description='Epoch:',\n",
    "                            value=0, min=0, max=99, step=1,\n",
    "                            layout=Layout(width='90%')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM on IWSLT2017 English to German Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc65bbd176e4a5db2c4eda2dd2ed5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Epoch:', layout=Layout(width='90%'), max=99), Output()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visual_func = visualize_translation_training('en_de', 'lstm', 'valid')\n",
    "_ = interact(visual_func, text_idx=IntSlider(description='Epoch:',\n",
    "                            value=0, min=0, max=99, step=1,\n",
    "                            layout=Layout(width='90%')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanh RNN on IWSLT2017 English to German Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8f1efa59ee4e8ab47ff9d894fb5d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Epoch:', layout=Layout(width='90%'), max=99), Output()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visual_func = visualize_translation_training('en_de', 'tanh', 'valid')\n",
    "_ = interact(visual_func, text_idx=IntSlider(description='Epoch:',\n",
    "                            value=0, min=0, max=99, step=1,\n",
    "                            layout=Layout(width='90%')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models have learned to output \"Und der\", \"Und das\", \"Und die\" for translating \"And the\". It seems that for latter epochs the Tanh RNN model have completely overfit and forgot about \"Und der\". It seems that LSTM and GRU are outputting similar gibberishes throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
